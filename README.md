
---

# Desafio beeM√¥n

Este √© um desafio proposto pela beeM√¥n.

## Descri√ß√£o do Projeto

O projeto tem como objetivo criar uma aplica√ß√£o que realize a sincroniza√ß√£o de informa√ß√µes sobre filmes, coletando dados a partir de uma fonte externa, utilizando t√©cnicas de web scraping. A aplica√ß√£o √© respons√°vel por buscar informa√ß√µes sobre produtos de um site espec√≠fico, processar esses dados e armazen√°-los em um banco de dados.

## Tecnologias Utilizadas

Para alcan√ßar esse objetivo, foram utilizadas as seguintes tecnologias:

  - Python
  - Django
  - Django REST Framework
  - PostgreSQL
  - Celery
  - Redis
  - Docker
  - Pandas
  - Requests
  - BeautifulSoup

## Instala√ß√£o e Uso

Para instalar e executar o projeto, siga as etapas abaixo:

1. Clone o reposit√≥rio para a sua m√°quina local:

   ```shell
   git clone git@github.com:AlbertoWagner/desafio-crawler.git
   ```

2. Navegue at√© o diret√≥rio do projeto:

   ```shell
   cd desafio-crawler
   ```

3. Configure as vari√°veis de ambiente no arquivo `dev.env`. Certifique-se de fornecer informa√ß√µes sens√≠veis, como chaves de acesso e senhas:

   ```
   SECRET_KEY=django-insecure-o$pcu!s$tbml==0fjfeo%$(92^!13hv#p7qckma)p03_t^*j&z
   DEBUG=True
   SITE_ID=1
   ALLOWED_HOSTS=*
   
   # Configura√ß√µes do banco de dados PostgreSQL
   DB_NAME=mydatabase
   DB_USER=myuser
   DB_PASSWORD=mypassword
   DB_HOST=db
   DB_PORT=5432
   
   # Configura√ß√µes do Celery e Redis
   DJANGO_SETTINGS_MODULE=desafio_crawler.settings
   CELERY_BROKER_URL=redis://redis:6379/0
   CELERY_RESULT_BACKEND=redis://redis:6379/0
   
   # Outras configura√ß√µes
   POSTGRES_HOST_AUTH_METHOD=trust
   POSTGRES_USER=myuser
   POSTGRES_PASSWORD=mypassword
   POSTGRES_DB=mydatabase
   
   # Configura√ß√µes do superusu√°rio do Django
   DJANGO_SUPERUSER_USERNAME=admin
   DJANGO_SUPERUSER_EMAIL=admin@example.com
   DJANGO_SUPERUSER_PASSWORD=admin
   ```

4. Crie e inicie os cont√™ineres Docker:

   ```shell
   docker-compose up --build
   ```

5. Aguarde at√© que todos os servi√ßos estejam em execu√ß√£o. Voc√™ ver√° logs indicando o progresso.

6. Acesse a aplica√ß√£o em seu navegador em http://localhost:8187/.

7. Para carregar as informa√ß√µes iniciais sobre os filmes:

   ```shell
   docker exec -it desafio-crawler-web-1 celery -A desafio_crawler call scraping.tasks.build_create_and_update_movies
   ```

8. Para executar os testes, utilize o seguinte comando:

   ```shell
   docker exec -it desafio-crawler-web-1 python manage.py test
   ```

## REST API

A aplica√ß√£o disponibiliza uma REST API para acessar e gerenciar informa√ß√µes sobre os filmes. Abaixo est√£o os principais endpoints da API:

- `GET /serializer/`: Retorna uma lista de filmes.

## Conte√∫do

- `GET /`: Retorna uma lista de filmes.

![Minha Imagem](iscreen_01.jpg)

- No painel de administra√ß√£o do Django, √© poss√≠vel acompanhar os registros de tarefas (tasks) realizadas.

![Minha Imagem](iscreen_02.jpg)

## Cron Job

Foi configurado um cron job para executar a sincroniza√ß√£o dos dados dos filmes em um hor√°rio espec√≠fico todos os dias, programado para rodar √†s 23:30.

O objetivo do cron job √© buscar os dados dos filmes a partir da fonte externa, process√°-los e armazen√°-los no banco de dados. Isso garante que a base de dados esteja sempre atualizada com as informa√ß√µes mais recentes.

Este projeto foi desenvolvido como parte do desafio proposto pela beeM√¥n.

<a href="#">
 <sub><b>Alberto Wagner</b></sub></a> <a href="#" ></a>

Feito por Alberto üëãüèΩ Entre em contato!

[![Linkedin Badge](https://img.shields.io/badge/-Alberto-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/alberto-wagner-5571a3106/)](https://www.linkedin.com/in/alberto-wagner-5571a3106/)
[![Gmail Badge](https://img.shields.io/badge/-albertow475@gmail.com-c14438?style=flat-square&logo=Gmail&logoColor=white&link=mailto:albertow475@gmail.com)](mailto:albertow475@gmail.com)

---
